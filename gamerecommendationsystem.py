# -*- coding: utf-8 -*-
"""GameRecommendationSystem.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hNGeYd7dvjw89TfGBJcYr7UV1W2mpJF_

# Recommendation System
# Game Recommendation with *content-based filtering*

*by: [Rifqi Novandi](https://github.com/rifqinvnd)*

## Background
In this machine learning project, a recommendation system model will be created to predict preferred games based on other games that have similar similarities or by using *content-based filtering* techniques with several variables such as platform, year of release, genre, etc.

## 1. Install and import the required libraries
"""

# Install the required library
!pip install -U scikit-learn
!pip install kaggle

# using os and zipfile library to prepare the dataset
import os
import zipfile
import json

# library for data processing
import pandas as pd
import numpy as np
from collections import Counter
from sklearn.preprocessing import MinMaxScaler

# library to make the recommendation system model
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics.pairwise import cosine_similarity

# library for evaluate the machine learning model
from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score

"""## 2. Prepares the Dataset

### 2.1 Prepare the username and kaggle key
"""

# prepares the Kaggle credential environment
os.environ['KAGGLE_USERNAME'] = 'rifqinovandi'
os.environ['KAGGLE_KEY'] = '61655b112a6218032cc7743aab07e371'

"""### 2.2 Download and prepare the dataset"""

# Download the dataset with Kaggle CLI
!kaggle datasets download -d rush4ratio/video-game-sales-with-ratings

# Extract zip file to CWD
files = "/content/video-game-sales-with-ratings.zip"
zip = zipfile.ZipFile(files, 'r')
zip.extractall('/content')
zip.close()

"""## 3. Data Understanding

### 3.1 Read data with pandas DataFrame
"""

df = pd.read_csv(files)
df.head()

"""## 3.2 Memahami isi keseluruhan dataset"""

df.shape

# Check dataset information
df.info()

# Check NaN value in columns
df.isna().sum()

# Describe dataset column
df.describe()

"""## 4. Data Preparation

### 4.1 Drop column that have missing values and unused
"""

df.drop(['Global_Sales', 'Critic_Score', 'Critic_Count', 'User_Count'], axis=1, inplace=True)

"""### 4.2 Clean every columns of the data

#### 4.2.1 Name column
"""

# Check missing value
df[df['Name'].isna()]

# Drop missing-value
for index in df[df['Name'].isna()].index:
  df.drop(index, axis=0, inplace=True)

# make sure the missing value has been deleted
if(df['Name'].isna().sum() == 0):
  print("There is no empty data in the Name column")
else:
  print("Missing value detected")

"""#### 4.2.2 Platform Column"""

# use collections Counter to check the sum of each platform column element
platform_counter = Counter(df['Platform'])
platform_counter

# removes columns with counts less than 350
platform_less_than_350 = ['2600', '3DO', 'DC', 'GB', 'GEN', 'GG', 'N64','NES', 'NG',
                          'PCFX', 'SAT', 'SCD', 'SNES', 'TG16', 'WS', 'WiiU', 'XOne']

df = df[~df['Platform'].isin(platform_less_than_350)]

# Check unique element
df['Platform'].unique()

"""#### 4.2.3 Genre Column"""

# Check missing value
df['Genre'].isna().sum()

# Check unique element
df['Genre'].unique()

# Check the sum of each unique element
genre_counter = Counter(df['Genre'])
genre_counter

# discard row with misc genre that is too complex
df = df[df['Genre'] != 'Misc']

# check dataset information
df.info()

"""#### 4.2.4 Publisher Column"""

# check missing-value
df['Publisher'].isna().sum()

# remove every riw with missing value
for index in df[df['Publisher'].isna()].index:
  df.drop(index, axis=0, inplace=True)

# recheck every missing value in the publisher column has been removed
if(df['Publisher'].isna().sum() == 0):
  print("There is no empty data in the Publisher column")
else:
  print("Missing value detected")

# Check unique element
df['Publisher'].unique()

# Check the unknown element
df[df['Publisher'] == 'Unknown']

# remove row with the unknown publisher
for index in df[df['Publisher'] == 'Unknown'].index:
  df.drop(index, axis=0, inplace=True)

"""#### 4.2.5 Year of Release Column"""

# check missing value
df['Year_of_Release'].isna().sum()

# remove missing value
for index in df[df['Year_of_Release'].isna()].index:
  df.drop(index, axis=0, inplace=True)

# ensure the missing value has been removed
if(df['Year_of_Release'].isna().sum() == 0):
  print("There is no empty data in Year_of_Release column")
else:
  print("Missing value detected")

# check unique element
df['Year_of_Release'].unique()

# change column type to string because it is categorical
df['Year_of_Release'] = df['Year_of_Release'].astype('str')

# check missing value of the whole data
df.isna().sum()

"""#### 4.2.6 User Score Column"""

# remove the missing value in the user score, developer, and rating columns
for index in df[df['User_Score'].isna()].index:
  df.drop(index, axis=0, inplace=True)

for index in df[df['Developer'].isna()].index:
  df.drop(index, axis=0, inplace=True)

for index in df[df['Rating'].isna()].index:
  df.drop(index, axis=0, inplace=True)

# check that all missing values in the dataset have been removed
if df.isna().sum().sum() == 0:
  print('Dataset cleaned')
else:
  print('Missing value detected')

# check unique element
df['User_Score'].unique()

# check tbd element in User_Score column
df[df['User_Score'] == 'tbd']

# remove row with tbd user score
for index in df[df['User_Score'] == 'tbd'].index:
  df.drop(index, axis=0, inplace=True)

# change the user score data type to float as a numerical feature
df['User_Score'] = df['User_Score'].astype('float')

"""#### 4.2.7 Developer Column"""

# check the number of different elements in the developer column
df['Developer'].nunique()

# because the number of different elements is too much and the column is categorical then the column is discarded
df.drop('Developer', axis=1, inplace=True)

"""#### 4.2.8 Rating Column"""

# check unique element
df['Rating'].unique()

"""### 4.3 Duplicate data cleaning"""

df.duplicated().sum()

df.info()

# The results of the data after the cleaning process
df = df.reset_index(drop=True)
df

df.describe()

"""### 4.4 Restructure data

#### 4.4.1 Creating a dataframe containing the game name
"""

# save game names on new dataframe
df_game_name = pd.DataFrame({'Game': df['Name']}).reset_index(drop=True)
df_game_name.head()

# use name column as index
df.set_index('Name', inplace=True)
df.head()

"""#### 4.4.2 Categorical label conversion with one-hot encoding"""

# select all columns with datatype object
column_object = df.dtypes[df.dtypes == 'object'].keys()
column_object

# convert category data to one-hot encoding
one_hot_label = pd.get_dummies(df[column_object])
one_hot_label.head(3)

# delete column with data type object
df.drop(column_object,axis=1,inplace=True)
df.head()

# unify one-hot encoding data with whole data
df = pd.concat([df,one_hot_label],axis=1)
df.head()

"""#### 4.4.3 Numerical column standardization"""

# select all numeric column
column_numeric = list(df.dtypes[df.dtypes == 'float64'].keys())
column_numeric

# MinMaxScaler initiation
scaler = MinMaxScaler()

# numerical column data standardization
scaled = scaler.fit_transform(df[column_numeric])

# scaled the data
i=0
for column in column_numeric:
    df[column] = scaled[:,i]
    i += 1

# check the result of the normalized data
df.head()

df.describe()

"""## 5. Create *Content-based Filtering* Recommendation System Model

### 5.1 Using the K-NearestNeighbors
"""

# Model initiation
model = NearestNeighbors(metric='euclidean')

# Fit model to the data
model.fit(df)

# Create function to get the game recommendation
def GameRecommended(gamename:str, recommended_games:int=6):
  print(f'If user like playing Game: \n{gamename[0]}\n5 Game that the user might like to play:')
  # Looking for the game with the highest similarity to the game that user play
  distances, neighbors = model.kneighbors(df.loc[gamename],n_neighbors=recommended_games)
  # Input the recommended game into the list
  similar_game = []
  for gamename in df_game_name.loc[neighbors[0][:]].values:
    similar_game.append(gamename[0])
  # Input the distance score into the list
  similar_distance = []
  for distance in distances[0]:
    similar_distance.append(f"{round(100-distance, 2)}%")
  # Return a dataframe with the most recommended game
  return pd.DataFrame(data = {"Game" : similar_game[1:], "Similarity" : similar_distance[1:]})

# Give the recommendation to the selected game
GameRecommended(df_game_name.loc[111])

"""### 5.2 Using Cosine Similarity"""

# Calculate the cosine similarity of the dataframe
cosine_sim = cosine_similarity(df)

# Keep the result of the calculation dataframe
cosine_sim_df = pd.DataFrame(cosine_sim, index=df_game_name['Game'], columns=df_game_name['Game'])
cosine_sim_df.head(3)

# Create function to get the game recommendation
def CosineGameRecommended(gamename:str, recommended_games:int=5):
  print(f'If user like playing Game: \n{gamename[0]}\n5 Game that the user might like to play:')
  # Look up the unique value of the game the user likes in the cosine sim dataframe row
  # The unique value (arr) is returned in an ordered form from small to large 
  arr, ind = np.unique(cosine_sim_df.loc[gamename[0]], return_index=True)
  # Input similar game names from the second-last index to the nth-last index
  similar_game = []
  for index in ind[-(recommended_games+1):-1]:
    similar_game.append(df_game_name.loc[index][0])
  # Input the cosine scores of similar games starting from the second-last index to the nth-last index.
  cosine_score = []
  for score in arr[-(recommended_games+1):-1]:
    cosine_score.append(score)
  # Return a dataframe with the most recommended game
  return pd.DataFrame(data = {"Game" : similar_game, "Cosine Similarity" : cosine_score}).sort_values(by='Cosine Similarity',ascending=False)

# provides recommendations with cosine similarity on selected games
CosineGameRecommended(df_game_name.loc[111])

"""## 6. Recommendation System Model Evaluation

### 6.1 Calinski Harabasz Score
"""

calinski_harabasz_score(df, df_game_name).round(2)

"""### 6.2 Davies Bouldin Score"""

davies_bouldin_score(df, df_game_name).round(2)

"""## Closing
A model for game recommendation with *content-based filtering* has been completed. After testing, the model works quite well in providing the top 5 recommendations for games that users might like/play. However, there are still some shortcomings of the model as seen in the Calinski Harabasz and Davies Bouldin scores. To improve it, algorithms can be used to create other recommendation models such as using deep learning or *collaborative filtering* and then compare its performance with the current KNN model.

### References
- Scikit-learn Docummentation: [https://scikit-learn.org/stable/modules/classes.html](https://scikit-learn.org/stable/modules/classes.html)
- Report References: [Contoh Algoritma Sistem Rekomendasi dengan Dokumentasi](https://github.com/fahmij8/ML-Exercise/blob/main/MLT-2/MLT_Proyek_Submission_2.ipynb)
- Dataset: [Game Sales with Rating Dataset](https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings)
"""